# NSR: The node status reporter scripts


## What are they?

The **node status reporter** scripts are scripts used to monitor the computers
that we have running our software.  Currently, the NSR scripts are available to 
report information only about the machines that we have SSH access too (these would
be the planet-lab machines, as well as the local UW instructional machines).  In the
future versions, these scripts will be able to monitor ALL the nodes we have by accessing
information from openDHT and the central server.

## How do I use them?

The scripts are located in the /deploymentscripts branch under trunk in the SVN.
Included in that directory are technically three components.  The first component 
consists of scripts that connect and run tests and grab logs from the remote 
computers, the second component parses the information from these logs, and then
the third component is the webserver which presents the information to a user.


### Preliminary Setup
In order to start using the scripts, you must first run a preparetest.py on a 
directory (or untar the files from a seattle install) as the scripts use some
of seattlelib and other files.  

The second step is to copy over all *.pubkey from trunk/seattlegeni/node_state_transitions/.
These files will be used to determine the states of the nodes.

We also need to setup ssh-agent so that it will be able to set up our RSA key
for identification to ssh/scp in batch mode without our interaction.  '''NOTE: this
means that the key you will be using must be added as a trusted key on the planetlab
machines.'''

In your console, run the following commands. The first command evaluates the 
output from ssh-agent, and the second will prompt you for the passphrases for 
all of your keys so that it will have them cached.

```
  eval `ssh-agent`
  ssh-add
```

Lastly, we must add two more files to the directory from which we will be executing
these scripts.  The first file is the **iplist2.list** file which has the IPs/hostnames
of all the computers that we'll be connecting to (See below for file format).  The
second file is a called ** hashes.dict ** and is generated by verifyfiles.py (you'll have to
run the verifyfiles.mix though repypp.py).

### Great, now that I'm all set up, what do I do?

After you've followed the instructions in setting up the files, it is very simple 
to run the scripts.  The **deploy_main.py** file is the file that executes all the
scripts and even makes the call the create the summary files.

**python deploy_main.py**

will execute the scripts and use the default iplist2.list filename to obtain the list of IPs.

## Using deploy_main.py


### What files/directories does deploy_main.py create locally?

deploy_main.py will create the following directories:

deploy.logs
  This is where the logs from the current run will go, while they're active.
detailed_logs
  This directory is technically created by make_summary.py, but it is called only
  after deploy_main.py has finished grabbing info from all the nodes it has to
  process.  In this directory, we have multiple folder names where each folder is
  the IP or hostname of the machine, and inside that folder there are multiple text
  files that have a unix time as their filename.  The unixtime is the time when the
  logs have begun to be generated (aka when make_summary.py has been executed).  
  Preserving this directory structure is important as the webserver looks for exactly
  this directory structure when it serves up web pages.
deploy.logs.X
  If deploy_main.py finds an existing deploy.logs folder, it will rename it to 
  deploy.logs.X where X is the next integer directory that does not exist and 
  then tar the directory and remove the untarred directory.

### What other options can I specify with deploy_main.py?

-c supplementary_file
  The -c command line will include an additional file (currently only python files
  are allowed to be included) and it will execute that file with python _.  At this 
  point, if you are wanting to add any type of code to be executed on each machine,
  add a method to custom.py and call it from custom.py's main.  custom.py is the file 
  that runs on all the machines right now and performs tasks such as grabbing info 
  from the v2 directory.
  
--nokeep
  Will not tarball the old deploy.logs directory, but will just remove it.
  

  
## What exactly gets executed on the remote machines?

When the scripts have connected to a machine they will clean up any old log
directories, and then execute runlocaltests.py.  This file attempts to locate
the seattle_repy (the seattle install) directory and then if it finds one then
it will execute the following two scripts:

verifyfiles.py
  Checks the checksum of the files and folders in that directory and will output
  any inconsistencies to stdout.
testprocess.py
  Checks that the Software Updater and the Node Manager are running and aren't 
  consuming too much memory, too many instances aren't running, and some other 
  checks.  See file for more information.
  
runlocaltests.py will grab all stdout and stderror from any and all scripts it 
executes.  runlocaltests.py will also execute a supplementary script if one was 
specified with the -c command line argument to deploy_main.py.


# Misc. Info


## Format for iplist files

Blank lines are allowed and will be ignored.  Lines starting with a # (pound) character
will also be ignored and are used as comment characters.  The first valid line must start
with a 

**!user:[username]**

where [username] (without square brackets) is the username that will be used with the
following list of nodes. Multiple user's can be specified. For example:

```
!user:konp
1.2.3.4
3.5.7.8

# begin a new user
!user:justinc
3.4.5.6
1.8.2.1
```

will connect to konp@1.2.3.4, konp@3.5.7.8, justinc@3.4.5.6, etc