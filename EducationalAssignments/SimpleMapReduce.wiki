[[TOC(inline)]]
----
== Overview ==

The original !MapReduce framework was designed and developed by Google to support large-scale distributed computing on large amounts of closely-networked computers.  The main drive for the !MapReduce framework stem from the intrinsic benefits of functional programming, more specifically the map() and reduce() functions.

There are 3 main steps in a !MapReduce run.

 1. Map step: data received by the worker is tossed into a map() function that will emit key-value pairs based on each aliquot of data.

 2. Partition (or shuffle) step: The generated key-value pairs from map() are hashed by its key to a particular worked and sent off.  If a key ends up on a particular work at the end of the partition phase, it is required that all other key-value pairs with the same key also be on the same host in order for the reduce step to work properly.

 3. Reduce step: The collected key-value pairs are collected into a key, value_1, value_2, ..., value_n list and can do computation to reduce the expanded data down to a useful conclusion.

This computation can be performed on an arbitrary number of worker machines, making it scalable to nearly all sizes of computing clusters!  !MapReduce frameworks often have a primary node delegating tasks to workers, organizing and distributing the initial data, and acting as a mediator in fault situations.

More detail on the use of !MapReduce is available from [http://labs.google.com/papers/!MapReduce.html the original Google paper] or an open-source implementation of !MapReduce in Java: [http://hadoop.apache.org/core/ Hadoop].

In this first assignment, a simple one worker node and one primary node !MapReduce framework will be built.  The primary node will acquire data, hand it off to the worker, and wait for the worker to finish.  The worker will perform a map() pass and a reduce() pass on the data and send the result back to the primary node.  Since only one worker is involved in the computation, the partition step is left for future assignments.  This code will serve as a base for the future assignments that expand the scalability and robustness of the framework.
[[BR]]
[[BR]]

== The Assignment ==
----
=== Step 1: Create the Worker code ===

For the worker, begin with implementing a pipeline.  Where is the worker going to call its mapping function and reduce function from?  Put these calls in the script portion of the '''mapred.repy''' file, after ''if callfunc == 'initialize':''.  First, the worker needs to call a map function, giving the initial data as a parameter.


== Hints ==
 * Hints to go here...

== Notes ==

 * Make sure you do not call your own functions map(), reduce(), or hash().  These are built-in python methods, consider using other related names!

== What to turn in ==

Turn in a tar of your repy code including the following files:

 * '''mappri.repy''' - Your primary node code
 * '''mapred.repy''' - Your worker node code
 * '''README''' - A readme file detailing any bugs, limitations, or special operating instructions of your program